{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kerasToOnnx1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNckdppAnl/PH4AW3Qku4eM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lorFdRoAk-v","executionInfo":{"status":"ok","timestamp":1607811231252,"user_tz":480,"elapsed":1274,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"9b6b30f5-0b2b-4b9d-beb9-628fdf2f9be6"},"source":["#here we requests the dataset from googleapis.com \r\n","#should return a zip file \r\n","!wget --no-check-certificate \\\r\n","    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\r\n","    -O /tmp/cats_and_dogs_filtered.zip\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-12-12 22:13:51--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.196.128, 108.177.112.128, 172.217.212.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.196.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 68606236 (65M) [application/zip]\n","Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n","\n","/tmp/cats_and_dogs_ 100%[===================>]  65.43M   154MB/s    in 0.4s    \n","\n","2020-12-12 22:13:51 (154 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UOEoY23KBJOs"},"source":["import os \r\n","import zipfile\r\n","#unzip  the zip file\r\n","local_zip = '/tmp/cats_and_dogs_filtered.zip'\r\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n","zip_ref.extractall('/tmp')\r\n","zip_ref.close()\r\n","\r\n","base_dir = '/tmp/cats_and_dogs_filtered'\r\n","train_dir = os.path.join(base_dir, 'train')\r\n","validation_dir = os.path.join(base_dir, 'validation')\r\n","\r\n","# Directory with our training cat pictures\r\n","train_cats_dir = os.path.join(train_dir, 'cats')\r\n","\r\n","# Directory with our training dog pictures\r\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\r\n","\r\n","# Directory with our validation cat pictures\r\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\r\n","\r\n","# Directory with our validation dog pictures\r\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqeIbjdkB49B","executionInfo":{"status":"ok","timestamp":1607812815084,"user_tz":480,"elapsed":851749,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"783b23ae-5d72-42d7-b007-44c431a0004c"},"source":["from tensorflow.keras import layers\r\n","from tensorflow.keras import Model\r\n","\r\n","# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\r\n","# the three color channels: R, G, and B\r\n","img_input = layers.Input(shape=(150, 150, 3))\r\n","\r\n","# First convolution extracts 16 filters that are 3x3\r\n","# Convolution is followed by max-pooling layer with a 2x2 window\r\n","x = layers.Conv2D(16, 3, activation='relu')(img_input)\r\n","x = layers.MaxPooling2D(2)(x)\r\n","\r\n","# Second convolution extracts 32 filters that are 3x3\r\n","# Convolution is followed by max-pooling layer with a 2x2 window\r\n","x = layers.Conv2D(32, 3, activation='relu')(x)\r\n","x = layers.MaxPooling2D(2)(x)\r\n","\r\n","# Third convolution extracts 64 filters that are 3x3\r\n","# Convolution is followed by max-pooling layer with a 2x2 window\r\n","x = layers.Conv2D(64, 3, activation='relu')(x)\r\n","x = layers.MaxPooling2D(2)(x)\r\n","\r\n","# Flatten feature map to a 1-dim tensor so we can add fully connected layers\r\n","x = layers.Flatten()(x)\r\n","\r\n","# Create a fully connected layer with ReLU activation and 512 hidden units\r\n","x = layers.Dense(512, activation='relu')(x)\r\n","\r\n","# Create output layer with a single node and sigmoid activation\r\n","output = layers.Dense(1, activation='sigmoid')(x)\r\n","\r\n","# Let's create model:\r\n","# input = input feature map\r\n","# output = input feature map + stacked convolution/maxpooling layers + fully \r\n","# connected layer + sigmoid output layer\r\n","keras_model = Model(img_input, output)\r\n","\r\n","#Let's compile\r\n","from tensorflow.keras.optimizers import RMSprop\r\n","\r\n","keras_model.compile(loss='binary_crossentropy',\r\n","              optimizer=RMSprop(lr=0.001),\r\n","              metrics=['acc'])\r\n","\r\n","\r\n","#Let's generate image data\r\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","\r\n","# All images will be rescaled by 1./255\r\n","train_datagen = ImageDataGenerator(rescale=1./255)\r\n","val_datagen = ImageDataGenerator(rescale=1./255)\r\n","\r\n","# Flow training images in batches of 20 using train_datagen generator\r\n","train_generator = train_datagen.flow_from_directory(\r\n","        train_dir,  # This is the source directory for training images\r\n","        target_size=(150, 150),  # All images will be resized to 150x150\r\n","        batch_size=20,\r\n","        # Since we use binary_crossentropy loss, we need binary labels\r\n","        class_mode='binary')\r\n","\r\n","# Flow validation images in batches of 20 using val_datagen generator\r\n","validation_generator = val_datagen.flow_from_directory(\r\n","        validation_dir,\r\n","        target_size=(150, 150),\r\n","        batch_size=20,\r\n","        class_mode='binary')\r\n","\r\n","\r\n","#let's train!\r\n","keras_history = keras_model.fit_generator(\r\n","      train_generator,\r\n","      steps_per_epoch=100,  # 2000 images = batch_size * steps\r\n","      epochs=15,\r\n","      validation_data=validation_generator,\r\n","      validation_steps=50,  # 1000 images = batch_size * steps\r\n","      verbose=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","WARNING:tensorflow:From <ipython-input-8-7a7a56def3a7>:76: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Epoch 1/15\n","100/100 - 56s - loss: 0.8066 - acc: 0.5620 - val_loss: 0.6729 - val_acc: 0.6230\n","Epoch 2/15\n","100/100 - 56s - loss: 0.6463 - acc: 0.6470 - val_loss: 0.6698 - val_acc: 0.5850\n","Epoch 3/15\n","100/100 - 56s - loss: 0.5705 - acc: 0.7025 - val_loss: 0.6055 - val_acc: 0.6850\n","Epoch 4/15\n","100/100 - 56s - loss: 0.4901 - acc: 0.7600 - val_loss: 0.6176 - val_acc: 0.6780\n","Epoch 5/15\n","100/100 - 56s - loss: 0.4209 - acc: 0.8000 - val_loss: 0.6266 - val_acc: 0.7060\n","Epoch 6/15\n","100/100 - 56s - loss: 0.3314 - acc: 0.8650 - val_loss: 0.6696 - val_acc: 0.6980\n","Epoch 7/15\n","100/100 - 56s - loss: 0.2788 - acc: 0.8880 - val_loss: 0.6392 - val_acc: 0.6990\n","Epoch 8/15\n","100/100 - 56s - loss: 0.1812 - acc: 0.9275 - val_loss: 0.7027 - val_acc: 0.7020\n","Epoch 9/15\n","100/100 - 56s - loss: 0.1203 - acc: 0.9495 - val_loss: 1.1166 - val_acc: 0.6980\n","Epoch 10/15\n","100/100 - 56s - loss: 0.0987 - acc: 0.9640 - val_loss: 1.7348 - val_acc: 0.6580\n","Epoch 11/15\n","100/100 - 56s - loss: 0.0717 - acc: 0.9805 - val_loss: 1.3421 - val_acc: 0.7220\n","Epoch 12/15\n","100/100 - 56s - loss: 0.0652 - acc: 0.9835 - val_loss: 1.4506 - val_acc: 0.7020\n","Epoch 13/15\n","100/100 - 56s - loss: 0.0616 - acc: 0.9860 - val_loss: 1.5851 - val_acc: 0.7340\n","Epoch 14/15\n","100/100 - 56s - loss: 0.0302 - acc: 0.9940 - val_loss: 1.9536 - val_acc: 0.7430\n","Epoch 15/15\n","100/100 - 56s - loss: 0.0807 - acc: 0.9840 - val_loss: 1.7663 - val_acc: 0.7300\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_B-cXg_I__f","executionInfo":{"status":"ok","timestamp":1607813314710,"user_tz":480,"elapsed":364,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"4dcc14b1-54d1-4eb8-a783-aaaf6a7f3535"},"source":["keras_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"functional_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 148, 148, 16)      448       \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 74, 74, 16)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 72, 72, 32)        4640      \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 36, 36, 32)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 17, 17, 64)        0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 18496)             0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 512)               9470464   \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 513       \n","=================================================================\n","Total params: 9,494,561\n","Trainable params: 9,494,561\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAbNxBnWIecb","executionInfo":{"status":"ok","timestamp":1607813173670,"user_tz":480,"elapsed":3642,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"f2ec8bbc-2f42-4ff3-cc56-d74ade9866eb"},"source":["pip install onnxruntime"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting onnxruntime\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/a9/f009251fd1b91a2e1ce6f22d4b5be9936fbd0072842c5087a2a49706c509/onnxruntime-1.6.0-cp36-cp36m-manylinux2014_x86_64.whl (4.1MB)\n","\u001b[K     |████████████████████████████████| 4.1MB 6.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (50.3.2)\n","Installing collected packages: onnxruntime\n","Successfully installed onnxruntime-1.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E7W8f-rDDAT3","executionInfo":{"status":"ok","timestamp":1607812955201,"user_tz":480,"elapsed":2828,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"6c298ee4-6caf-4fd7-8fc4-350b8923f95d"},"source":["pip install keras2onnx"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras2onnx in /usr/local/lib/python3.6/dist-packages (1.7.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.18.5)\n","Requirement already satisfied: onnxconverter-common>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.7.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (2.23.0)\n","Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (0.3.1)\n","Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from keras2onnx) (1.8.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (50.3.2)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->keras2onnx) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras2onnx) (3.0.4)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->keras2onnx) (1.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx->keras2onnx) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9q5s9OyuDUMg","executionInfo":{"status":"ok","timestamp":1607813734202,"user_tz":480,"elapsed":2933,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"261027c4-ab09-4dda-ee50-212820d0caeb"},"source":["import numpy as np\r\n","from keras.preprocessing import image\r\n","from keras.applications.resnet50 import preprocess_input\r\n","import keras2onnx\r\n","import onnxruntime\r\n","\r\n","onnx_model = keras2onnx.convert_keras(keras_model, keras_model.name)\r\n","\r\n","# runtime prediction\r\n","#content = onnx_model.SerializeToString()\r\n","\r\n","#sess = onnxruntime.InferenceSession(content)\r\n","#x = x if isinstance(x, list) else [x]\r\n","#feed = dict([(input.name, x[n]) for n, input in enumerate(sess.get_inputs())])\r\n","#pred_onnx = sess.run(None, feed)\r\n","temp_model_file = 'model.onnx'\r\n","keras2onnx.save_model(onnx_model, temp_model_file)\r\n","sess = onnxruntime.InferenceSession(temp_model_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf executing eager_mode: True\n","tf.keras model eager_mode: False\n","The ONNX operator number change on the optimization: 35 -> 18\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZcFCv15K3CR","executionInfo":{"status":"ok","timestamp":1607813792781,"user_tz":480,"elapsed":361,"user":{"displayName":"Julian Jaramillo","photoUrl":"","userId":"11320654170439150944"}},"outputId":"f00a4294-dd47-417a-cef7-9c7d27756991"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["model.onnx  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]}]}